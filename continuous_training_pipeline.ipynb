{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "continuous_training_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPckZTcDaVENUvd1P2YdIUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/pose-estimation/blob/main/continuous_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHHN6ZRT3rl9",
        "outputId": "5a779cca-e92d-4b26-eab8-58a478c90458"
      },
      "source": [
        "# 해당 셀을 실행한 후에 반드시 \"런타임 다시시작\"을 해주세요\n",
        "!pip install -q kfp\n",
        "!pip3 install --user kfp google-cloud-aiplatform matplotlib --upgrade -q\n",
        "# 추가\n",
        "!pip3 install --user google-cloud-aiplatform --upgrade -q\n",
        "!pip3 install --user kfp google-cloud-pipeline-components --upgrade -q"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 19.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 196 kB 60.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 196 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 196 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 195 kB 57.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 194 kB 57.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 193 kB 58.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 189 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 188 kB 55.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 188 kB 52.8 MB/s \n",
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.18.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 81 kB 4.1 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFaOAS8639SU"
      },
      "source": [
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
        "                        OutputPath, component, ClassificationMetrics, Metrics)\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "from kfp.v2.google import experimental\n",
        "from google.cloud import aiplatform\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pIyhIZa3_Jz"
      },
      "source": [
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user() # 사용할 gcp 계정으로 연결해주세요"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsJCkin74BTF"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ID = 'natural-expanse-319203'\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"gs://mediapipe-pipeline\"\n",
        "\n",
        "USER = \"JeongMin-Do\"\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10gbboRQ4M42"
      },
      "source": [
        "@component(base_image=\"silverstar456/mediapipe:landmarks\")\n",
        "def get_landmarks(result_csv: OutputPath(\"result_csv\")):\n",
        "    import csv\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import sys\n",
        "    import tqdm\n",
        "    import pandas as pd\n",
        "\n",
        "    from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
        "    from mediapipe.python.solutions import pose as mp_pose\n",
        "\n",
        "    import wget \n",
        "    import zipfile\n",
        "\n",
        "    # GCS에서 이미지 파일 다운로드 \n",
        "    wget.download(\"https://storage.googleapis.com/mediapipe-pipeline/study.zip\")\n",
        "\n",
        "    # 압축해제\n",
        "    fantasy_zip = zipfile.ZipFile('study.zip')\n",
        "    fantasy_zip.extractall('.')\n",
        "    fantasy_zip.close()\n",
        "\n",
        "    def landmarks(input_frame):\n",
        "        input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Initialize fresh pose tracker and run it.\n",
        "        with mp_pose.Pose(upper_body_only=True) as pose_tracker:\n",
        "            result = pose_tracker.process(image=input_frame)\n",
        "            pose_landmarks = result.pose_landmarks\n",
        "        \n",
        "        # Save landmarks.\n",
        "        if pose_landmarks is not None:\n",
        "            # Check the number of landmarks and take pose landmarks.\n",
        "            assert len(pose_landmarks.landmark) == 25, 'Unexpected number of predicted pose landmarks: {}'.format(len(pose_landmarks.landmark))\n",
        "            pose_landmarks = [[lmk.x, lmk.y, lmk.z] for lmk in pose_landmarks.landmark]\n",
        "\n",
        "            # Map pose landmarks from [0, 1] range to absolute coordinates to get\n",
        "            # correct aspect ratio.\n",
        "            frame_height, frame_width = input_frame.shape[:2]\n",
        "            pose_landmarks *= np.array([frame_width, frame_height, frame_width])\n",
        "\n",
        "            # Write pose sample to CSV.\n",
        "            pose_landmarks = np.around(pose_landmarks, 5).flatten().astype(np.float64).tolist()\n",
        "\n",
        "            return pose_landmarks\n",
        "\n",
        "    images_in_folder = 'study'\n",
        "    csv_out_path = result_csv\n",
        "\n",
        "    with open(csv_out_path, 'w') as csv_out_file:\n",
        "        csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "        # Folder names are used as pose class names.\n",
        "        pose_class_names = sorted([n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
        "\n",
        "        for pose_class_name in pose_class_names:\n",
        "            print('Bootstrapping ', pose_class_name, file=sys.stderr)\n",
        "\n",
        "            image_names = sorted([\n",
        "                n for n in os.listdir(os.path.join(images_in_folder, pose_class_name))\n",
        "                if not n.startswith('.')])\n",
        "            for image_name in tqdm.tqdm(image_names, position=0):\n",
        "                # Load image.\n",
        "                input_frame = cv2.imread(os.path.join(images_in_folder, pose_class_name, image_name))\n",
        "                pose_landmarks = landmarks(input_frame)\n",
        "\n",
        "                try:\n",
        "                    csv_out_writer.writerow([image_name, pose_class_name] + pose_landmarks)\n",
        "                except:\n",
        "                    pass       "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0loVE7Wj_gzZ"
      },
      "source": [
        "@component(base_image=\"silverstar456/mediapipe:knn\")\n",
        "def knn(result_csv: InputPath(\"result_csv\"), model_output: OutputPath(\"model\")):\n",
        "    import pandas as pd\n",
        "\n",
        "    import pickle\n",
        "    from joblib import dump, load\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "    data = pd.read_csv(result_csv, header=None)\n",
        "    x_train, y_train = data.iloc[:, 2:], data.iloc[:, 1]\n",
        "\n",
        "    classifier = KNeighborsClassifier(n_neighbors = 3)\n",
        "\n",
        "    classifier.fit(x_train, y_train)\n",
        "\n",
        "    dump(classifier, model_output) \n",
        "\n",
        "    # clf = load('filename.joblib') "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "7Y0J81gpv5Nz",
        "outputId": "1ca451a3-3ce5-4a4c-f5eb-7d87fb0f75af"
      },
      "source": [
        "@dsl.pipeline(\n",
        "    name = \"mediapipe-pipeline\",\n",
        "    description = \"mediapipe\",\n",
        "    pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def mediapipe():\n",
        "    landmarks = get_landmarks()\n",
        "    model = knn(landmarks.output)\n",
        "\n",
        "\n",
        "    model_upload_op = gcc_aip.ModelUploadOp(\n",
        "        project=PROJECT_ID, # 프로젝트 명\n",
        "        display_name=\"mediapipe-pose\", # 모델 이름 설정\n",
        "        artifact_uri='gs://mediapipe-pipeline/pipeline_root/JeongMin-Do', # 모델이 저장된 directory\n",
        "        serving_container_image_uri= \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\", # serving시 사용할 컨테이너 이미지\n",
        "    )\n",
        "    model_upload_op.after(model)\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func = mediapipe, \n",
        "    package_path = \"mediapipe.json\"\n",
        ")\n",
        "\n",
        "api_client = AIPlatformClient(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        ")\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "    job_spec_path=\"mediapipe.json\"\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/kfp/v2/google/client/client.py:175: FutureWarning: AIPlatformClient will be deprecated in v1.9. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mediapipe-pipeline-20210807152446?project=natural-expanse-319203\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b79QUMAR0ARa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A78-pIjR0AOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLqlJ7bi2zJ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}